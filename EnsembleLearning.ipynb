{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45030350-1745-4b36-8550-16edcf811657",
   "metadata": {},
   "source": [
    "# ENSEMBLE LEARNING\n",
    "It is an ml techquie that envolves combining multiple individual models known as base learns to make predictions and desicions.    \n",
    "The main idea behind ensemble learning is that by combinig the predictions of multiple models the overall performnace can be improved compared to using an single model.     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3759e98-10ed-4e98-be4b-4d15ae6d1a1c",
   "metadata": {},
   "source": [
    "# TECHNIQUES\n",
    "(1).Bagging ---> Bagging stands for bootstrap aggregating it involves cretaing multiple subsets of the training data through bootstraping .      \n",
    "                training an seperate base learner on each subset and then combining there predictions.    \n",
    "                the most common example of bagging is RandomForest Algorithmnwhich combines multiple dicision trees.   \n",
    "\n",
    "                \n",
    "(2).Boosting --->Boosting is an iterative ensemnble method that focus on training weak learns squentially and giving more importance to the instances that were misclassified by the pervious learners.    \n",
    "                In boosting each base learner is trainer to correct the mistakes made by the pervious learners"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dacb3ba-a97d-4837-8f66-24bf2a417472",
   "metadata": {},
   "source": [
    "# ADVANTAGES\n",
    "(1). Improved Performces--> ensemble methods can ofen achieve better performance then individual models specially when the base learns are diverse and complimentary.\n",
    "\n",
    "(2).ROBUSTNESS --> Ensemble learning can be more robust to noisy data and outliers bcz error(loss function) made by indivdual models can be componstated by others.  \n",
    "\n",
    "(3)Reducing Overfitting---> Ensemble method can help reduce overfitting as an combination of multiple models reduces the risk of relying too heavily on an single model.    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec622a4-b7d2-4933-ba17-577940f611bc",
   "metadata": {},
   "source": [
    "# 1.BAGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e52856d-d499-471c-85c3-2a938029f94c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Accuracy: 0.935\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=11)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=11)\n",
    "\n",
    "# Initialize a list to store the base learners\n",
    "base_learners = []\n",
    "\n",
    "#Numbers of base learners(decision trees)\n",
    "num_base_learners = 10\n",
    "\n",
    "#Train the base learners\n",
    "for i in range(num_base_learners):\n",
    "    #Create a bootstrap sample of the training data\n",
    "    bootstrap_indices = np.random.choice(len(X_train) , size = len(X_train) , replace = True)\n",
    "    X_bootstrap = X_train[bootstrap_indices]\n",
    "    y_bootstrap = y_train[bootstrap_indices]\n",
    "\n",
    "    #Creating and training a base learner (RandomForest)\n",
    "    base_learner = RandomForestClassifier(n_estimators=10, random_state=42)\n",
    "    base_learner. fit(X_bootstrap, y_bootstrap)\n",
    "\n",
    "    # Add the trained base learner to the list\n",
    "    base_learners.append(base_learner)\n",
    "\n",
    "# Make predictions with each base Learner\n",
    "base_predictions = []\n",
    "for base_learner in base_learners:\n",
    "    y_pred = base_learner.predict(X_test)\n",
    "    base_predictions.append(y_pred)\n",
    "\n",
    "# Combine the predictions using majority voting\n",
    "ensemble_predictions = np.round(np.mean(base_predictions, axis=0))\n",
    "\n",
    "# Calculate the accuracy of the ensemble predictions\n",
    "accuracy = accuracy_score(y_test, ensemble_predictions)\n",
    "print(\"Ensemble Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f981e34-0b9d-45f2-80dd-5bc314e1f09b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
